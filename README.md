# Multi-Modal Sentiment Analysis System

> **Complete end-to-end pipeline for understanding human emotions through Text, Audio, and Visual inputs**

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://multi-modal-sentiment-analysis.streamlit.app/)

A production-ready system with **Attention-Based Fusion** for Human-Robot Interaction applications.

## âœ¨ Key Features

- ğŸ¤– **RoBERTa** Text Encoder - understanding words
- ğŸ¤ **Wav2Vec2** Audio Encoder - detecting tone and emotion
- ğŸ‘ï¸ **ViT** Visual Encoder - reading facial expressions
- ğŸ§  **Multi-Head Attention Fusion** - intelligent cross-modal reasoning
- ğŸ“Š **Attention Visualization** - interpretable predictions
- ğŸš€ **Interactive Web App** - multi-page Streamlit interface
- ğŸ“ˆ **Complete Training Pipeline** - data loading, training, inference
- ğŸ“ **Custom Dataset Support** - Use your own data (see [DATASET_GUIDE.md](DATASET_GUIDE.md))

## ğŸ—ï¸ Architecture

![System Architecture Diagram](docs/architecture.png)

```
src/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ config.py              # Configuration
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoders/
â”‚   â”‚   â”œâ”€â”€ base.py            # Abstract encoder
â”‚   â”‚   â”œâ”€â”€ text.py            # RoBERTa
â”‚   â”‚   â”œâ”€â”€ audio.py           # Wav2Vec2
â”‚   â”‚   â””â”€â”€ visual.py          # ViT
â”‚   â”œâ”€â”€ fusion/
â”‚   â”‚   â””â”€â”€ attention.py       # Multi-Head Attention
â”‚   â””â”€â”€ system.py              # Main system
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.py             # Data loading
â”œâ”€â”€ training/
â”‚   â””â”€â”€ trainer.py             # Training loop
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ predictor.py           # Inference
â””â”€â”€ utils/
    â””â”€â”€ visualization.py       # Plotting utils
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Train the Model (Optional - works with untrained model too)
```bash
python train.py
```

This will:
- Create dummy training data automatically
- Train for 10 epochs
- Save best model to `models/best_model.pt`
- Save training history to `models/training_history.json`

### 3. Launch Web App (Local)
```bash
streamlit run app.py
```

**ğŸŒ Live Demo:** [multi-modal-sentiment-analysis.streamlit.app](https://multi-modal-sentiment-analysis.streamlit.app/)

The app has 3 pages:
- **ğŸ”® Prediction** - Analyze sentiment from multimodal inputs
- **ğŸ“Š Training Info** - View training curves and metrics
- **â„¹ï¸ About** - Learn about the architecture

### 4. Test Architecture (Optional)
```bash
python test_new_architecture.py
```

### 5. Deploy with Docker (Production)
```bash
# Build and run with Docker Compose
docker-compose up -d

# Access at http://localhost:8501
```

See [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md) for detailed deployment guide.

## ğŸ“Š Complete Pipeline

### Data Preprocessing
```python
from src.data import MultiModalDataset, collate_fn
from torch.utils.data import DataLoader

# Create dataset
dataset = MultiModalDataset(data_dir='data', split='train')

# Create data loader
loader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)
```

### Training
```python
from src.models import MultiModalSentimentSystem
from src.training import Trainer

# Initialize model
model = MultiModalSentimentSystem(freeze_encoders=True)

# Train
trainer = Trainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    device='cuda',
    num_epochs=10
)
trainer.train()
```

### Inference
```python
from src.pipeline import Predictor

# Load trained model
predictor = Predictor(checkpoint_path='models/best_model.pt')

# Predict
result = predictor.predict_single(
    text="I love this!",
    audio=audio_waveform,
    image=pil_image
)

print(result['sentiment'])  # 'positive'
print(result['attention_weights'])  # Shows modality importance
```

## ğŸ¯ How It Works

### Attention-Based Fusion vs Late Fusion

**âŒ Late Fusion (Old):**
```
Text â†’ Logits â”€â”€â”
Audio â†’ Logits â”€â”¼â”€â†’ Average â†’ Prediction
Visual â†’ Logits â”˜
```
Problem: Modalities work independently, can't detect sarcasm.

**âœ… Attention Fusion (New):**
```
Text â†’ Features (768) â”€â”€â”
Audio â†’ Features (768) â”€â”¼â”€â†’ Multi-Head Attention â†’ Fused Features â†’ Prediction
Visual â†’ Features (768) â”˜                              â†“
                                                 Attention Weights
```
Advantage: Cross-modal communication enables nuanced understanding.

## ğŸ“ˆ Training Details

The system trains using:
- **Loss**: Cross-Entropy Loss
- **Optimizer**: AdamW (lr=1e-4, weight_decay=0.01)
- **Scheduler**: Cosine Annealing
- **Batch Size**: 4
- **Epochs**: 10 (configurable)

Training outputs:
- `models/best_model.pt` - Best checkpoint (highest validation accuracy)
- `models/final_model.pt` - Final checkpoint
- `models/training_history.json` - Metrics history

## ğŸ–¼ï¸ Streamlit App Features

### Prediction Page
- Upload text, audio (WAV/MP3), and images (JPG/PNG)
- Real-time sentiment analysis
- Confidence scores
- Attention heatmap visualization
- Modality importance bar chart

### Training Info Page
- Training/validation loss curves
- Training/validation accuracy curves
- Final metrics display

### About Page
- Architecture explanation
- Use cases
- Technical details

## ğŸ“ Real-World Example

**Scenario: Detecting Sarcasm**

```
Input:
- Text: "Great job, you really nailed it."
- Audio: Sarcastic tone (negative)
- Visual: Eye roll (negative)

Late Fusion: "Positive" (only sees positive words)
Attention Fusion: "Negative" (weighs audio + visual more)
```

The attention weights would show:
- Text: 0.15 (low importance)
- Audio: 0.50 (high importance)
- Visual: 0.35 (moderate importance)

This is why attention-based fusion is essential for Human-Robot Interaction.

## ğŸ”§ Configuration

Edit `src/core/config.py` to customize:
- Model dimensions
- Training hyperparameters
- Attention heads
- Dropout rates

## ğŸ“¦ Project Structure

```
Multi Modal-Sentiment Analysis/
â”œâ”€â”€ src/                    # Source code (modular architecture)
â”‚   â”œâ”€â”€ core/              # Configuration
â”‚   â”œâ”€â”€ models/            # Encoders + Fusion
â”‚   â”œâ”€â”€ data/              # Dataset loading
â”‚   â”œâ”€â”€ training/          # Training pipeline
â”‚   â”œâ”€â”€ pipeline/          # Inference
â”‚   â””â”€â”€ utils/             # Visualization
â”œâ”€â”€ src_legacy/            # Old code (archived)
â”œâ”€â”€ data/                  # Datasets (auto-created if missing)
â”œâ”€â”€ models/                # Saved checkpoints
â”œâ”€â”€ app.py                 # Streamlit web app
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ test_new_architecture.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âœ… Resume Claims - All Implemented

| Claim | Implementation |
|-------|----------------|
| RoBERTa for Text | âœ… [`src/models/encoders/text.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/src/models/encoders/text.py) |
| Wav2Vec2 for Audio | âœ… [`src/models/encoders/audio.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/src/models/encoders/audio.py) |
| ViT for Visual | âœ… [`src/models/encoders/visual.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/src/models/encoders/visual.py) |
| Attention-Based Fusion | âœ… [`src/models/fusion/attention.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/src/models/fusion/attention.py) |
| Attention Visualization | âœ… [`src/utils/visualization.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/src/utils/visualization.py) |
| Interactive Streamlit App | âœ… [`app.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/app.py) (3-page interface) |
| Training Pipeline | âœ… [`train.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/train.py) + [`src/training/trainer.py`](file:///c:/Users/Akhil/Python_Projects/ML/Multi%20Modal-Sentiment%20Analysis/src/training/trainer.py) |

## ğŸ“„ License

MIT License - Free for educational and commercial use!

## ğŸ™ Acknowledgments

- HuggingFace for Transformers library
- PyTorch team
- Streamlit for the web framework
